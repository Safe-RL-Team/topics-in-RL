**This is a curated list of safe RL papers from 2017 to 2022. If you would like to contribute additional papers or update the list, please feel free to do so.**

## Our Journey of Reimplementing Safe RL Algorithms

Reimplementing state-of-the-art RL algorithms allows us to gain a deeper understanding of the algorithms' inner workings, and subsequently, explore novel and innovative approaches. [Safe Reinforcement Learning](https://rongrg.github.io/posts/2023-04-12-saferl/) is a cutting-edge field that holds immense potential for real-world applications. 

During the course ["Advanced Topics in Reinforcement Learning"](https://rongrg.github.io/teaching/2022-winter-teaching-4/), we took on the challenge of reimplementing ideas from several recent safe RL papers.

Our findings and discussions are available as scientific blogs, 
with code re-implementations available on our GitHub repository (https://github.com/Safe-RL-Team).

Join us on an exciting journey of advancing the field of Safe RL! 

1. [Safe Reinforcement Learning via Curriculum Induction](https://arxiv.org/pdf/2006.12136.pdf), Matteo Turchetta, Andrey Kolobov, Shital Shah, Andreas Krause, and Alekh Agarwal

    ðŸ“š [Blog](https://safe-rl-team.github.io/curriculum-learning/) Marvin Sextro, Jonas Loos

2. [Safe Reinforcement Learning with Natural Language Constraints](https://proceedings.neurips.cc/paper/2021/file/72f67e70f6b7cdc4cc893edaddf0c4c6-Paper.pdf), Tsung-Yen Yang, Michael Hu, Yinlam Chow, Peter J. Ramadge, and Karthik Narasimhan, NeurIPS 2021

    ðŸ“š [Blog](https://safe-rl-team.github.io/SRL-NLC-Report/)   Hongyou Zhou

3. [Adversarial Policies: Attacking Deep Reinforcement Learning](https://openreview.net/pdf?id=HJgEMpVFwB), Adam Gleave, Michael Dennis, Cody Wild, Neel Kant, Sergey Levine, and Stuart Russell, ICLR 2020

    ðŸ“š [Blog](https://safe-rl-team.github.io/adversarial-policies-pytorch-blog/)   Lorenz Hufe, Jarek Liesen

4. [Reward constrained policy optimization](https://openreview.net/pdf?id=SkfrvsA9FX), Chen Tessler, Daniel J. Mankowitz, and Shie Mannor, ICLR 2019

    ðŸ“š [Blog](https://iclr-blogposts.github.io/staging/blog/2023/Adaptive-Reward-Penalty-in-Safe-Reinforcement-Learning/)   Boris Meinardus, Tuan Anh Le

5. [Constrained Policy Optimization via Bayesian World Models](https://arxiv.org/pdf/2201.09802.pdf), Yarden As, Ilnura Usmanova, Sebastian Curi and Andreas Krause, ICLR 2022

    ðŸ“š [Blog](https://safe-rl-team.github.io/lambda-bo-blog/)   Vincent Meilinger

6. [Constrained Policy Optimization](http://proceedings.mlr.press/v70/achiam17a/achiam17a.pdf), Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel, ICML 2017

    ðŸ“š [Blog](https://safe-rl-team.github.io/CPO-Blog/)   Thanh Cuong Le, Paul Hasenbusch

7. [Responsive Safety in Reinforcement Learning by PID Lagrangian Methods Adam](http://proceedings.mlr.press/v119/stooke20a/stooke20a.pdf), Adam Stooke, Joshua Achiam, and Pieter Abbeel, ICML 2020

    ðŸ“š [Blog](https://safe-rl-team.github.io/PID/)   Wenxi Huang

8. [There Is No Turning Back: A Self-Supervised Approach for Reversibility-Aware Reinforcement Learning](https://openreview.net/pdf?id=3X65eaS4PtP), Nathan Grinsztajn, Johan Ferret, Olivier Pietquin, Philippe Preux, and Matthieu Geist, NeurIPS 2021

    ðŸ“š [Blog](https://safe-rl-team.github.io/Blog-Post-about-There-is-No-Turning-Back/There%20is%20No%20Turning%20Back%20A%20Self-Supervised%20Approach%20for%20Reversibility-Aware%20Reinforcement%20Learning.html)   Malik-Manel Hashim

9. [Uncertainty-Based Offline Reinforcement Learning with Diversified Q-Ensemble](https://proceedings.neurips.cc/paper/2021/file/3d3d286a8d153a4a58156d0e02d8570c-Paper.pdf), Gaon An, Seungyong Moon, Jang-Hyun Kim, and Hyun Oh Song, NeurIPS 2021

    ðŸ“š [Blog](https://safe-rl-team.github.io/Uncertainty-Based-Offline-RL-with-Diversified-Q-Ensemble/)   Jonas Loos, Julian Dralle

10. [Learning Barrier Certificates: Towards Safe Reinforcement Learning with Zero Training-time Violations](https://openreview.net/pdf?id=K4Su8BIivap), Yuping Luo, and Tengyu Ma, NeurIPS 2021

    ðŸ“š [Blog](https://safe-rl-team.github.io/barrier-certificates/)   Lars Chen, Jeremiah Flannery

11. [Teachable Reinforcement Learning via Advice Distillation](https://proceedings.neurips.cc/paper/2021/file/37cfff3c04f95b22bcf166df586cd7a9-Paper.pdf), Olivia Watkins, Trevor Darrell, Pieter Abbeel, Jacob Andreas, and Abhishek Gupta, NeurIPS 2021

    ðŸ“š [Blog](https://safe-rl-team.github.io/advice-distillation-blog/)   Mihai Dumitrescu, Claire Sturgill

12. [Cautious Adaptation For Reinforcement Learning in Safety-Critical Settings](http://proceedings.mlr.press/v119/zhang20e/zhang20e.pdf), Jesse Zhang, Brian Cheung, Chelsea Finn, Sergey Levine, and Dinesh Jayaraman, ICML 2020

    ðŸ“š [Blog](https://safe-rl-team.github.io/CARL/)   Maren Eberle

13. [Verifiable Reinforcement Learning via Policy Extraction](https://arxiv.org/pdf/1805.08328.pdf), Osbert Bastani, Yewen Pu, and Armando Solar-Lezama, NeurIPS 2018

    ðŸ“š [Blog](https://safe-rl-team.github.io/viper-verifiable-reinforcement-learning/)   Christoph PrÃ¶schel

14. [Time Discretization-Invariant Safe Action Repetition for Policy Gradient Methods](https://proceedings.neurips.cc/paper/2021/file/024677efb8e4aee2eaeef17b54695bbe-Paper.pdf), Seohong Park, Jaekyeom Kim, and Gunhee Kim, NeurIPS 2021

    ðŸ“š [Blog](https://safe-rl-team.github.io/safe-action-repetition-article/)   Hristo Boyadzhiev


## Pushing Boundaries and Prioritizing Safety in RL

By implementing and exploring ideas from state-of-the-art papers, we can push the boundaries of what is possible 
and pave the way for even more effective and robust safe RL algorithms. 

So, let's dive in and make the world a safer place, one policy at a time!


## Reference
1. [GarcÃ­a, J. and Fernandez, F., A Comprehensive Survey on Safe Reinforcement Learning *Journal of Machine Learning Research, 2015*](https://www.jmlr.org/papers/volume16/garcia15a/garcia15a.pdf)
2. [Ray, A., Achiam, J. and Amodei, D., Benchmarking Safe Exploration in Deep Reinforcement Learning *Open AI, 2019*](https://cdn.openai.com/safexp-short.pdf)
3. [Kumar, A. and Levine, S, Offline Reinforcement Learning: From Algorithms to Practical Challenges *NeurIPS Tutorial 2020*](https://sites.google.com/view/offlinerltutorial-neurips2020/home?authuser=0)
